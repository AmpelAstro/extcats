{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingetsing larger catalogs: PanStarrs DR1 Mean/Thin Objects\n",
    "\n",
    "This notebook follows up on *insert_example*, be sure to go through that one first.\n",
    "\n",
    "Here we show a possible way in which the catalog ingestion step can be sped up though the use of mulitprocessing. In particular, we will try to import some of the files from a subset and compilation of PS1 DR1 mean and thin object tables. The entire subsample contains 1.92B sources, selected requiring nDetections>2.\n",
    "\n",
    "Reference: https://panstarrs.stsci.edu/\n",
    "\n",
    "The test files we will be using for this test can be downloaded from:\n",
    "\n",
    "https://desycloud.desy.de/index.php/s/stCkA6uJ8ayKvjI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import CatalogPusher\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from healpy import ang2pix\n",
    "\n",
    "import importlib\n",
    "importlib.reload(CatalogPusher)\n",
    "\n",
    "# build the pusher object and point it to the raw files.\n",
    "ps1p = CatalogPusher.CatalogPusher(\n",
    "    catalog_name = 'ps1_test',                    # short name of the catalog\n",
    "    data_source = '../testdata/PS1DR1_test/',     # where to find the data (other options are possible)\n",
    "    file_type = '*.csv.gz'                        # filter files (there is col definition file in data_source)\n",
    "    )\n",
    "\n",
    "\n",
    "# define the reader for the raw files (import column names from file.)\n",
    "headfile = '../testdata/PS1DR1_test/column_headings.csv'\n",
    "with open(headfile, 'r') as header:\n",
    "    catcols=[c.strip() for c in header.readline().split(',')]\n",
    "ps1p.assign_file_reader(\n",
    "        reader_func = pd.read_csv,           # callable to use to read the raw_files. \n",
    "        read_chunks = True,                  # weather or not the reader process each file into smaller chunks.\n",
    "        names=catcols,                       # All other arguments are passed directly to this function.\n",
    "        chunksize=50000,\n",
    "        engine='c')\n",
    "\n",
    "\n",
    "# define modifier. This time the healpix grid is finer\n",
    "hp_nside12=2**12\n",
    "def ps1_modifier(srcdict):\n",
    "    ra=srcdict['raMean'] if srcdict['raMean']<180. else srcdict['raMean']-360.\n",
    "    srcdict['pos']={\n",
    "            'type': 'Point', \n",
    "            'coordinates': [ra, srcdict['decMean']]\n",
    "                    }\n",
    "    srcdict['hpxid_12']=int(\n",
    "        ang2pix(hp_nside12, srcdict['raMean'], srcdict['decMean'], lonlat = True, nest = True))\n",
    "    return srcdict\n",
    "ps1p.assign_dict_modifier(ps1_modifier)\n",
    "\n",
    "\n",
    "# wrap up the file pushing function so that we can \n",
    "# use multiprocessing to speed up the catalog ingestion\n",
    "def pushfiles(filerange):\n",
    "    ps1p.push_to_db(\n",
    "        coll_name = 'srcs',\n",
    "        index_on = ['hpxid_12'],\n",
    "        filerange = filerange,\n",
    "        overwrite_coll = False,\n",
    "        dry = False)\n",
    "    \n",
    "\n",
    "# each job will run on a subgroup of all the files\n",
    "file_groups = ps1p.file_groups(group_size=4)\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers = 2) as executor:\n",
    "    executor.map(pushfiles, file_groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
